{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tensor Network Contractions using `np.einsum`\n",
    "\n",
    "In this notebook, you will learn how to use NumPy's `np.einsum` function to perform **tensor network contractions**. We'll cover the basics of `np.einsum`, how to use it for complex contractions, and how to compute the **optimal contraction path** for improved runtime performance.\n",
    "\n",
    "## Table of Contents\n",
    "1. What is `np.einsum`?\n",
    "2. Basic Usage of `np.einsum`\n",
    "3. Tensor Network Contractions with `np.einsum`\n",
    "4. Computing the Optimal Path with `np.einsum_path`\n",
    "5. The `optimize` parameter\n",
    "6. Opt_einsum\n",
    "7. Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is `np.einsum`?\n",
    "\n",
    "The function `np.einsum` (Einstein Summation) is a powerful tool in NumPy that allows for complex array operations, including:\n",
    "\n",
    "- Summation over multiple axes\n",
    "- Transposing arrays\n",
    "- Inner/Outer products\n",
    "- Generic Matrix and tensor contractions\n",
    "\n",
    "The syntax of `np.einsum` uses Einstein summation notation to specify how tensors should be contracted or manipulated. This concise notation eliminates the need for explicit loops or intermediate operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Usage of `np.einsum`\n",
    "\n",
    "Let's start with a few simple examples to understand how `np.einsum` works.\n",
    "\n",
    "### Example 1: Matrix Multiplication\n",
    "\n",
    "![](../img/matmul.png)\n",
    "\n",
    "Matrix multiplication between two 2D arrays can be performed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of matrix multiplication using einsum:\n",
      " [[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary library\n",
    "import numpy as np\n",
    "\n",
    "# Define two matrices\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Perform matrix multiplication using einsum\n",
    "C = np.einsum('ij,jk->ik', A, B)\n",
    "print('Result of matrix multiplication using einsum:\\n', C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string `'ij,jk->ik'` tells `np.einsum` how to contract the tensors:\n",
    "- The first matrix `A` is represented by the indices `ij`.\n",
    "- The second matrix `B` is represented by the indices `jk`.\n",
    "- The resulting matrix `C` is indexed by `ik`, meaning that the index `j` is summed over (contracted).\n",
    "\n",
    "This corresponds to the traditional definition of matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Element-wise Summation\n",
    "\n",
    "![](../img/trace.png)\n",
    "\n",
    "You can also use `np.einsum` for element-wise operations. For example, summing over all elements of a matrix can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of all elements in A: 10\n"
     ]
    }
   ],
   "source": [
    "# Define a matrix\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Sum all elements using einsum\n",
    "sum_A = np.einsum('ij->', A)\n",
    "print('Sum of all elements in A:', sum_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notation `'ij->'` indicates that the indices `i` and `j` should be summed over, resulting in a scalar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tensor Network Contractions with `np.einsum`\n",
    "\n",
    "Tensor networks consist of nodes (tensors) connected by edges, which represent the contraction of indices between tensors. `np.einsum` can efficiently handle these contractions.\n",
    "\n",
    "Let's perform a more complex contraction between three tensors.\n",
    "\n",
    "![](../img/contract3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of tensor network contraction:\n",
      " [[2.95215913 2.90979933 1.83302395 1.4888609  1.56921193]\n",
      " [3.50377242 3.41674896 2.23219287 1.63371673 1.81712126]]\n"
     ]
    }
   ],
   "source": [
    "# Index dimensions\n",
    "Di = 30\n",
    "Dj = 50\n",
    "Dk = 20\n",
    "Dl = 50\n",
    "Dm = 20\n",
    "# Define three tensors\n",
    "A = np.random.rand(Di, Dj, Dk) # A_ijk\n",
    "B = np.random.rand(Dj, Dl) # B_jl\n",
    "C = np.random.rand(Dk, Dm) # C_km\n",
    "\n",
    "# Perform a contraction over multiple tensors\n",
    "D = np.einsum('ijk,jl,km->ilm', A, B, C) # D_ilm\n",
    "print('Result of tensor network contraction:\\n', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are contracting the following indices:\n",
    "- `A[i,j,k]` is contracted with `B[j,l]` over index `j`.\n",
    "- `A[i,j,k]` is contracted with `C[k,m]` over index `k`.\n",
    "\n",
    "This is a typical tensor network contraction performed with `np.einsum`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Computing the Optimal Path with `np.einsum_path`\n",
    "\n",
    "When contracting large tensor networks, the order of contraction can significantly affect performance. **`np.einsum_path`** can be used to compute the optimal contraction path that minimizes both computational cost and memory usage.\n",
    "\n",
    "The function `np.einsum_path` outputs optimal contraction order along with information including the scaling and the size of the largest intermediate tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Contraction Path: ['einsum_path', (0, 1), (0, 1)]\n",
      "\n",
      "Detailed Path Information:\n",
      "  Complete contraction:  ijk,jl,km->ilm\n",
      "         Naive scaling:  5\n",
      "     Optimized scaling:  4\n",
      "      Naive FLOP count:  9.000e+07\n",
      "  Optimized FLOP count:  4.200e+06\n",
      "   Theoretical speedup:  21.429\n",
      "  Largest intermediate:  3.000e+04 elements\n",
      "--------------------------------------------------------------------------\n",
      "scaling                  current                                remaining\n",
      "--------------------------------------------------------------------------\n",
      "   4                 jl,ijk->kil                              km,kil->ilm\n",
      "   4                 kil,km->ilm                                 ilm->ilm\n"
     ]
    }
   ],
   "source": [
    "# Compute the optimal path for contracting three tensors\n",
    "opt_path, path_info = np.einsum_path('ijk,jl,km->ilm', A, B, C, optimize='optimal')\n",
    "print('Optimal Contraction Path:', opt_path)\n",
    "print('\\nDetailed Path Information:')\n",
    "print(path_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Output:\n",
    "1. **Optimal Contraction Path**: This lists the optimal order of contraction, minimizing computational complexity.\n",
    "2. **Path Information**: This includes detailed information about each contraction step, the scaling and memory usage.\n",
    "\n",
    "This optimal path computation can significantly improve runtime when contracting large tensor networks, especially when the tensors are high-dimensional or very large.\n",
    "\n",
    "### Breakdown of Path Information\n",
    "\n",
    "- **Naive Scaling**: The naive runtime scaling: this will just be the number of indices involved in the contraction.\n",
    "\n",
    "- **Optimized Scaling**: The runtime scaling with the optimized contraction order.\n",
    "\n",
    "- **Largest intermediate**: This tells you the size (number of elements) of the largest intermediate tensor that is computed during the contraction. Large intermediate tensors can be a source of inefficiency, as they require more memory and may lead to slower performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The `optimize` parameter\n",
    "\n",
    "The `optimize` parameter in `np.einsum` controls how the contraction path is computed. Different settings can lead to different execution times, particularly for large tensor networks.\n",
    "\n",
    "We can compare contraction times for different values of `optimize`:\n",
    "\n",
    "- `optimize=False`: No optimization, performs contractions as specified.\n",
    "- `optimize='greedy'`: Greedy optimization to reduce contraction cost, often faster but not always optimal.\n",
    "- `optimize='optimal'`: Uses dynamic programming to find the optimal contraction path. This can be more expensive to compute but often results in the fastest runtime for large tensors.\n",
    "- `optimize=path`: We manually set the contraction path. We can use the output of `np.einsum_path` to set the optimal path. This should be the fastest option as the optimal path has already been computed in advance.\n",
    "\n",
    "We'll compare these values by timing a tensor contraction using each option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.1 ms ± 693 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# 1. Optimize=False\n",
    "result_no_opt = np.einsum('ijk,jl,km->ilm', A, B, C, optimize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 μs ± 12.3 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# 2. Optimize='greedy'\n",
    "result_greedy_opt = np.einsum('ijk,jl,km->ilm', A, B, C, optimize='greedy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 μs ± 20.8 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# 3. Optimize='optimal'\n",
    "result_optimal_opt = np.einsum('ijk,jl,km->ilm', A, B, C, optimize='optimal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 μs ± 4.67 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# 4. Optimize=opt_path\n",
    "result_optimal_opt = np.einsum('ijk,jl,km->ilm', A, B, C, optimize=opt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that setting the precomputed optimal path is the fastest. The slowest is `optimize=False` because this uses the sub-optimal path (scaling 5). Using the greedy algorithm is ever so slighly faster than 'optimal' since the path computed is the same but computing the path takes less time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Opt_einsum\n",
    "\n",
    "We have chosen to explore tensor contraction using NumPy's einsum function, however it is important to point out that is not the most performant option out there. For a faster and more feature rich function check out opt_einsum:\n",
    "\n",
    "https://optimized-einsum.readthedocs.io/en/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises\n",
    "\n",
    "Now that you understand the basics of using `np.einsum` and how to compute the optimal contraction path, try the following exercises to test your comprehension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Matrix Transposition and Multiplication\n",
    "1. Create two matrices `A` of shape `(3, 2)` and `B` of shape `(3, )` filled with random integers.\n",
    "2. Use `np.einsum` to compute the transpose of matrix `A` and then multiply it by matrix `B`.\n",
    "3. Verify the result using standard matrix operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Outer Product\n",
    "1. Create two 1D arrays `x` and `y` of length 4.\n",
    "2. Use `np.einsum` to compute the outer product of `x` and `y`.\n",
    "3. Verify the result using `np.outer` and compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Tensor Contractions\n",
    "\n",
    "For tensor networks 1, 2 and 3.\n",
    "\n",
    "![](../img/contract_ex.png)\n",
    "a. What is the order of the resulting tensor? Eg, scalar, matrix, ...\n",
    "\n",
    "b. What is the optimal runtime scaling of the contraction\n",
    "\n",
    "c. Create arrays corresponding to the tensors filled with random numbers.\n",
    "\n",
    "d. Use `np.einsum` to contract the network.\n",
    "\n",
    "e. Verify your answer to part a. by inspecting the result of `np.einsum`\n",
    "\n",
    "f. Verfiy your answer to part b. by inspecting the result of `np.einsum_path`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
